[
{
	"uri": "/",
	"title": "ContainerLabs",
	"tags": [],
	"description": "",
	"content": "AWS Container Labs: 容器实践动手训练营 容器服务概览  下图所示为 AWS 的容器服务概览（请关注此文件的持续更新版）。 "
},
{
	"uri": "/0-prepare.html",
	"title": "0-环境准备",
	"tags": [],
	"description": "",
	"content": " 请务必保证先按顺序做完本章里面的两个小节。\n 实验介绍  通过本次容器实践动手训练营，您可以了解如何利用AWS平台上丰富的容器服务能力，来快速构建现代化应用。\n"
},
{
	"uri": "/0-prepare/001-vpc.html",
	"title": "001-准备基础设施",
	"tags": [],
	"description": "",
	"content": " 这是这个Workshop的第一步，先准备网络和管理工具。\n 实验描述  本动手实验部署创建VPC，通过使用CloudFormation在实验环境中部署以下内容：\n网络基础\n 创建一个包含公有、私有子网的VPC 公有子网: 分别位于两个不同的可用区，该子网内的资源会暴露在互联网上，可被用户或客户端直接访问。用于部署NAT Gateway, 堡垒机，ELB负载均衡器等。 私有子网: 分别位于两个不同的可用区，该子网内的资源无法直接被互联网上的用户直接访问。用于部署Web应用服务器，中间件服务，数据库服务等无需直接暴露在互联网上的服务。 多个安全组: 用于控制EKS集群/EC2网络传入和传出流量。 Network ACL: 网络访问控制列表，可用作防火墙来控制进出一个或多个子网的流量。 Cloud9: 云IDE，便于在云上直接管理相关集群。  部署步骤   下载如下的yaml文件到本地操作的电脑客户端     部署VPC的CloudFormation模板文件   infra.yaml (12 )       打开 CloudFormation 控制台，并创建新的堆栈：\n  在“准备模板”部分，保留缺省的“模板已就绪”选项。然后在\u0026quot;模板源\u0026quot;部分，选择\u0026quot;上传模板文件\u0026quot;单选框，点击【选择文件】按钮，并选择第一步中下载的CloudFormation yaml文件。点击【下一步】按钮。 输入以下的相关的参数，并点击【下一步】按钮。\n   堆栈名称：输入ContainerWorkshop    其他选项按照默认即可，点击【下一步】按钮。   在“配置堆栈选项”页面上，保留缺省值，并点击【下一步】按钮。 在“审核”页面上，勾选“我确认，AWS CloudFormation 可能创建具有自定义名称的 IAM 资源。”\n  点击【创建堆栈】按钮，并等待此模板的创建执行完毕，此过程大概约5分钟左右。\n   在等待资源创建的过程中，可以先往下创建IAM角色 创建完成后，输出内容如下：   查看资源  进入VPC控制台： https://console.aws.amazon.com/vpc/home?region=us-east-1#vpcs:\n 查看创建好的VPC，子网，路由表，安全组，Internet Gateway，NAT Gateway等资源。  IAM配置  进入IAM控制台：https://console.aws.amazon.com/iam/home?region=us-east-1#/roles   创建角色 ContainerAdminRole 附加权限 AdministratorAccess   Cloud9环境配置  AWS Cloud9 是一个基于云的集成开发环境 (IDE)，您只需使用浏览器即可编写、运行和调试代码。它包括代码编辑器、调试器和终端。Cloud9 预先打包了适用于常用编程语言的基本工具和预安装的 AWS 命令行工具 (CLI)。\n  进入Cloud9控制台，点击第一步CloudFormation输出里的“Cloud9URL”，或者 https://console.aws.amazon.com/cloud9/home?region=us-east-1 刚部署好的 cloud9 干净环境如下：   点击CLoud9环境中右上角的图标，选择管理EC2实例（Manange EC Instance）\n  在接下来打开的窗口中，关联Cloud9的EC2实例和刚才创建的角色\n   此处为：ContainerAdminRole，如下图所示（仅供参考，因为每个人部署的实例ID不一样）    关联后保存即可   仍然在EC2控制台，操作更改安全组   添加三个安全组后保存\n   搜索 ControlPlaneSecurityGroup 搜索 SharedNodeSecurityGroup 搜索 ExternalSecurityGroup 以ControlPlaneSecurityGroup为例   最后绑定安全组类似如下（其中第一个是默认的）： 这里仅仅是为了实验方便，所以Cloud9 IDE 承担了很多角色，为了方便连通各个服务才绑定这么多安全组\n 更新Cloud9的环境配置   在Cloud9环境中，选择右上角的设置按钮，关闭：AWS managed temporary credentials   然后删除临时的credentials   在Cloud9 IDE 下方的命令行窗口执行以下命令  rm -vf ${HOME}/.aws/credentials "
},
{
	"uri": "/0-prepare/002-mgmt.html",
	"title": "002-准备管理环境",
	"tags": [],
	"description": "",
	"content": " 这是这个Workshop的第二步，准备管理环境。\n 实验描述  本动手实验基于Cloud9 IDE，安装需要的工具。\n 安装环境组件  echo \u0026#34;install libs\u0026#34; sudo yum -y install jq gettext bash-completion moreutils 更新 awscli 并配置自动完成  mv /bin/aws /bin/aws1 ls -l /usr/local/bin/aws curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install which aws_completer echo $SHELL cat \u0026gt;\u0026gt; ~/.bash_profile \u0026lt;\u0026lt;EOF complete -C \u0026#39;/usr/local/bin/aws_completer\u0026#39; aws EOF source ~/.bash_profile aws --version 安装 kubectl 并配置自动完成  curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl cat \u0026gt;\u0026gt; ~/.bash_profile \u0026lt;\u0026lt;EOF source \u0026lt;(kubectl completion bash) alias k=kubectl complete -F __start_kubectl k EOF source ~/.bash_profile kubectl version --client  检查 kubectl 安装   安装 helm  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh helm version helm repo add stable https://charts.helm.sh/stable 配置对应的属性信息   在Cloud9 IDE 下方的命令行窗口执行以下命令  export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) test -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region source ~/.bash_profile aws sts get-caller-identity 返回类似下面的字样表示配置成功（里面的 ContainerAdminRole 为我们前面步骤创建的） 一定要确保这一步成功，否则不要往下操作，请检查EC2的Role的相关配置。\n 增加磁盘空间（Cloud 9 默认只有10G），准备扩容脚本，新建文件resize-ebs.sh   复制内容到resize-ebs.sh  #!/bin/bash  # Specify the desired volume size in GiB as a command line argument. If not specified, default to 20 GiB. SIZE=${1:-20} # Get the ID of the environment host Amazon EC2 instance. INSTANCEID=$(curl http://169.254.169.254/latest/meta-data/instance-id) # Get the ID of the Amazon EBS volume associated with the instance. VOLUMEID=$(aws ec2 describe-instances \\  --instance-id $INSTANCEID \\  --query \u0026#34;Reservations[0].Instances[0].BlockDeviceMappings[0].Ebs.VolumeId\u0026#34; \\  --output text) # Resize the EBS volume. aws ec2 modify-volume --volume-id $VOLUMEID --size $SIZE # Wait for the resize to finish. while [ \\  \u0026#34;$(aws ec2 describe-volumes-modifications \\  --volume-id $VOLUMEID \\  --filters Name=modification-state,Values=\u0026#34;optimizing\u0026#34;,\u0026#34;completed\u0026#34; \\  --query \u0026#34;length(VolumesModifications)\u0026#34;\\  --output text)\u0026#34; != \u0026#34;1\u0026#34; ]; do sleep 1 done #Check if we\u0026#39;re on an NVMe filesystem if [ $(readlink -f /dev/xvda) = \u0026#34;/dev/xvda\u0026#34; ] then # Rewrite the partition table so that the partition takes up all the space that it can. sudo growpart /dev/xvda 1 # Expand the size of the file system. # Check if we are on AL2 STR=$(cat /etc/os-release) SUB=\u0026#34;VERSION_ID=\\\u0026#34;2\\\u0026#34;\u0026#34; if [[ \u0026#34;$STR\u0026#34; == *\u0026#34;$SUB\u0026#34;* ]] then sudo xfs_growfs -d / else sudo resize2fs /dev/xvda1 fi else # Rewrite the partition table so that the partition takes up all the space that it can. sudo growpart /dev/nvme0n1 1 # Expand the size of the file system. # Check if we\u0026#39;re on AL2 STR=$(cat /etc/os-release) SUB=\u0026#34;VERSION_ID=\\\u0026#34;2\\\u0026#34;\u0026#34; if [[ \u0026#34;$STR\u0026#34; == *\u0026#34;$SUB\u0026#34;* ]] then sudo xfs_growfs -d / else sudo resize2fs /dev/nvme0n1p1 fi fi  授权后执行，扩容到50G  chmod +x resize-ebs.sh ./resize-ebs.sh 50 配置SSH   自己生成key（不用输入什么，直接连续按3次回车即可）  ssh-keygen -t rsa -b 4096 然后再Cloud9导入  aws ec2 import-key-pair --key-name \u0026quot;container\u0026quot; --public-key-material fileb://~/.ssh/id_rsa.pub  如果上面导入失败，可以尝试用如下命令\n aws ec2 import-key-pair --key-name \u0026quot;container\u0026quot; --public-key-material file://~/.ssh/id_rsa.pub 导入成功会显示类似如下的内容： "
},
{
	"uri": "/1-management.html",
	"title": "1-Management",
	"tags": [],
	"description": "",
	"content": "实验内容  本次实验会在 Management 进行实践。本次实验包括：\n EKS 迁移实践 服务暴露 DNS集成 弹性伸缩 安全防护  "
},
{
	"uri": "/1-management/101-eks.html",
	"title": "101-EKS 迁移实践",
	"tags": [],
	"description": "",
	"content": "概述  在本Workshop中，您将构建两个k8s集群：\n 基于kind构建的本地集群 基于EKS的托管集群  借助开源和AWS提供的工具来进行模拟迁移实验。\n"
},
{
	"uri": "/1-management/102-exposing.html",
	"title": "102-服务暴露",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/1-management/103-dns.html",
	"title": "103-DNS集成",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/1-management/104-scaling.html",
	"title": "104-弹性伸缩",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/1-management/105-security.html",
	"title": "105-安全防护",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/1-management/104-scaling/1-hpa.html",
	"title": "A-应用水平弹性伸缩",
	"tags": ["Auto Scaling"],
	"description": "",
	"content": "介绍  EKS的弹性伸缩包括两个方面的内容:\n 一个是水平的 Pod 弹性伸缩（HPA：Horizontal Pod Autoscaler），这是调用K8s的API实现replica配置 一个是集群的 Node 弹性伸缩（CA：Cluster Autoscaler），这是集群层面的垂直弹性伸缩  部署 Metrics Server  参考官网 https://github.com/kubernetes-sigs/metrics-server\n 安装当前（截止到2021年6月）最新版 0.5.0  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml 检查安装情况  kubectl get apiservice v1beta1.metrics.k8s.io -o json | jq '.status' 类似输出如下：\n{ \u0026quot;conditions\u0026quot;: [ { \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2021-06-25T15:15:28Z\u0026quot;, \u0026quot;message\u0026quot;: \u0026quot;all checks passed\u0026quot;, \u0026quot;reason\u0026quot;: \u0026quot;Passed\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Available\u0026quot; } ] } Pod伸缩(HPA)   准备工作 确认 Metrics Server 部署成功，有数据才能实现 查看是否OK  kubectl top node 如果返回类似如下表示运行正常，可以考虑部署应用测试了\nNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% ip-192-168-115-52.ec2.internal 157m 8% 1068Mi 15% ip-192-168-39-196.ec2.internal 43m 2% 517Mi 7% ip-192-168-80-184.ec2.internal 76m 3% 743Mi 10% 部署测试环境  我们部署一个apache\nkubectl create deployment php-apache --image=us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests=cpu=200m kubectl expose deploy php-apache --port 80 kubectl get pod -l app=php-apache 设定cpu超过50%就扩容\nkubectl autoscale deployment php-apache `#The target average CPU utilization` \\ --cpu-percent=50 \\ --min=1 `#The lower limit for the number of pods that can be set by the autoscaler` \\ --max=10 `#The upper limit for the number of pods that can be set by the autoscaler` 查看状态\nkubectl get hpa 类似输出如下\nNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache \u0026lt;unknown\u0026gt;/50% 1 10 0 5s 大概过上1-2分钟，再次执行kubectl get hpa，我们就会发现 \u0026lt;unknown\u0026gt;/50% 变成了 0%/50% 。\nNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 73s 人为加压  借助开源的 apache ab 工具\nkubectl run apache-bench -ti --image=httpd --rm -- ab -n 300000 -c 1000 http://php-apache/  注意不要漏掉 http://php-apache/ 最后的 /\n 另开一个窗口，查看加压情况\nkubectl get hpa -w 发现：\n 负载上来后，逐步扩容，直到最大Pod数10 模拟加压停止后，再慢慢缩回  NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 72s php-apache Deployment/php-apache 242%/50% 1 10 1 77s php-apache Deployment/php-apache 937%/50% 1 10 4 93s php-apache Deployment/php-apache 431%/50% 1 10 8 108s php-apache Deployment/php-apache 189%/50% 1 10 10 2m3s php-apache Deployment/php-apache 190%/50% 1 10 10 2m19s php-apache Deployment/php-apache 185%/50% 1 10 10 2m34s php-apache Deployment/php-apache 186%/50% 1 10 10 2m50s php-apache Deployment/php-apache 174%/50% 1 10 10 3m5s php-apache Deployment/php-apache 64%/50% 1 10 10 3m21s php-apache Deployment/php-apache 100%/50% 1 10 10 3m35s php-apache Deployment/php-apache 96%/50% 1 10 10 3m51s php-apache Deployment/php-apache 94%/50% 1 10 10 4m5s php-apache Deployment/php-apache 47%/50% 1 10 10 4m21s php-apache Deployment/php-apache 0%/50% 1 10 10 4m35s 清理应用  通过如下方式清除刚才的水平扩容测试应用\nkubectl delete hpa,svc php-apache kubectl delete deployment php-apache "
},
{
	"uri": "/1-management/104-scaling/2-cluster-autoscaler.html",
	"title": "B-集群弹性伸缩",
	"tags": ["Auto Scaling"],
	"description": "",
	"content": "我们将结合 kube-ops-view （通过Helm部署，官方地址 ）来实现弹性伸缩的可视化配置的数据源。\n安装kube-ops-view   此处我们使用参数 --set service.type=LoadBalancer 表示把kube-ops-view的入口部署到ELB上（从而避免了必须使用kube-proxy做端口转发）  helm install kube-ops-view \\ stable/kube-ops-view \\ --set service.type=LoadBalancer \\ --set rbac.create=True 查看安装情况  helm list 如果返回类似如下表示部署成功\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION kube-ops-view default 1 2021-06-25 14:24:03.611338556 +0000 UTC deployed kube-ops-view-1.2.4 20.4.0 查看入口  kubectl get svc kube-ops-view | tail -n 1 | awk '{ print \u0026quot;Kube-ops-view URL = http://\u0026quot;$4 }' 系统会返回类似的结果\nKube-ops-view URL = http://a368d8f0ea6ab422eabb8445818b09fd-265230446.us-east-1.elb.amazonaws.com 打开其中的 URL（因为此处我们使用了ELB，在创建ELB和传播DNS的时候需要点时间，一般需要1-2分钟左右），会获得一个如下图所示的监控数据页面\n监控截图的含义如下 Cluster伸缩(CA)  在EKS里面，集群自动伸缩是集成了AWS的Auto Scaling Groups（ASG）服务实现的，支持如下几种方式\n 单个ASG 多个ASG 自动发现，点击这里查看更多详情 控制平台节点配置  参考官网 https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md\n 配置弹性伸缩组  查看系统已经部署好的asg的内容\naws autoscaling \\ describe-auto-scaling-groups \\ --query \u0026quot;AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') \u0026amp;\u0026amp; Value=='ekslab']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026quot; \\ --output table 如下所示（默认配置最小1个，最大和实际需要的实例数量都是3个）\n------------------------------------------------------------- | DescribeAutoScalingGroups | +-------------------------------------------+----+----+-----+ | eks-2abd20a6-2880-5ca0-bdc4-71eb19df6135 | 1 | 3 | 3 | +-------------------------------------------+----+----+-----+ 此处我们把最小实例数量调整为3个，最大的实例数量调整成5个\n# we need the ASG name export ASG_NAME=$(aws autoscaling describe-auto-scaling-groups --query \u0026quot;AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') \u0026amp;\u0026amp; Value=='ekslab']].AutoScalingGroupName\u0026quot; --output text) # increase max capacity up to 5 aws autoscaling \\ update-auto-scaling-group \\ --auto-scaling-group-name ${ASG_NAME} \\ --min-size 3 \\ --desired-capacity 3 \\ --max-size 5 # Check new values aws autoscaling \\ describe-auto-scaling-groups \\ --query \u0026quot;AutoScalingGroups[? Tags[? (Key=='eks:cluster-name') \u0026amp;\u0026amp; Value=='ekslab']].[AutoScalingGroupName, MinSize, MaxSize,DesiredCapacity]\u0026quot; \\ --output table 配置IAM角色  此处我们使用 IAM Roles for Service Accounts，有兴趣的读者请参考 官方文档\n使用如下的方式启用IAM角色和Service Accounts的功能\neksctl utils associate-iam-oidc-provider \\ --cluster ekslab \\ --approve 配置IAM角色\nmkdir ~/environment/cluster-autoscaler \u0026amp;\u0026amp; cd ~/environment/cluster-autoscaler cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/cluster-autoscaler/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:DescribeLaunchConfigurations\u0026quot;, \u0026quot;autoscaling:DescribeTags\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot;, \u0026quot;ec2:DescribeLaunchTemplateVersions\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam create-policy \\ --policy-name k8s-asg-policy \\ --policy-document file://~/environment/cluster-autoscaler/k8s-asg-policy.json 把IAM Role和SA关联起来\neksctl create iamserviceaccount \\ --name cluster-autoscaler \\ --namespace kube-system \\ --cluster ekslab \\ --attach-policy-arn \u0026quot;arn:aws:iam::${ACCOUNT_ID}:policy/k8s-asg-policy\u0026quot; \\ --approve \\ --override-existing-serviceaccounts 查看并确认\nkubectl -n kube-system describe sa cluster-autoscaler 部署 Cluster Autoscaler (CA)  准备部署配置文件\ncd ~/environment/cluster-autoscaler cat \u0026gt; cluster-autoscaler-autodiscover.yaml \u0026lt;\u0026lt;EOF --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-autoscaler labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;events\u0026quot;, \u0026quot;endpoints\u0026quot;] verbs: [\u0026quot;create\u0026quot;, \u0026quot;patch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods/eviction\u0026quot;] verbs: [\u0026quot;create\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;pods/status\u0026quot;] verbs: [\u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;endpoints\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;nodes\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: - \u0026quot;pods\u0026quot; - \u0026quot;services\u0026quot; - \u0026quot;replicationcontrollers\u0026quot; - \u0026quot;persistentvolumeclaims\u0026quot; - \u0026quot;persistentvolumes\u0026quot; verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;extensions\u0026quot;] resources: [\u0026quot;replicasets\u0026quot;, \u0026quot;daemonsets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;policy\u0026quot;] resources: [\u0026quot;poddisruptionbudgets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;] - apiGroups: [\u0026quot;apps\u0026quot;] resources: [\u0026quot;statefulsets\u0026quot;, \u0026quot;replicasets\u0026quot;, \u0026quot;daemonsets\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;storage.k8s.io\u0026quot;] resources: [\u0026quot;storageclasses\u0026quot;, \u0026quot;csinodes\u0026quot;] verbs: [\u0026quot;watch\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;get\u0026quot;] - apiGroups: [\u0026quot;batch\u0026quot;, \u0026quot;extensions\u0026quot;] resources: [\u0026quot;jobs\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;watch\u0026quot;, \u0026quot;patch\u0026quot;] - apiGroups: [\u0026quot;coordination.k8s.io\u0026quot;] resources: [\u0026quot;leases\u0026quot;] verbs: [\u0026quot;create\u0026quot;] - apiGroups: [\u0026quot;coordination.k8s.io\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler\u0026quot;] resources: [\u0026quot;leases\u0026quot;] verbs: [\u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: cluster-autoscaler namespace: kube-system labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler rules: - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] verbs: [\u0026quot;create\u0026quot;,\u0026quot;list\u0026quot;,\u0026quot;watch\u0026quot;] - apiGroups: [\u0026quot;\u0026quot;] resources: [\u0026quot;configmaps\u0026quot;] resourceNames: [\u0026quot;cluster-autoscaler-status\u0026quot;, \u0026quot;cluster-autoscaler-priority-expander\u0026quot;] verbs: [\u0026quot;delete\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-autoscaler labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: cluster-autoscaler namespace: kube-system labels: k8s-addon: cluster-autoscaler.addons.k8s.io k8s-app: cluster-autoscaler roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cluster-autoscaler subjects: - kind: ServiceAccount name: cluster-autoscaler namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscaler spec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler annotations: prometheus.io/scrape: 'true' prometheus.io/port: '8085' spec: serviceAccountName: cluster-autoscaler containers: - image: k8s.gcr.io/cluster-autoscaler:v1.20.0 name: cluster-autoscaler resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --expander=least-waste - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/ekslab - --balance-similar-node-groups - --skip-nodes-with-system-pods=false volumeMounts: - name: ssl-certs mountPath: /etc/ssl/certs/ca-certificates.crt readOnly: true imagePullPolicy: \u0026quot;Always\u0026quot; volumes: - name: ssl-certs hostPath: path: \u0026quot;/etc/ssl/certs/ca-bundle.crt\u0026quot; EOF 执行部署\nkubectl apply -f cluster-autoscaler-autodiscover.yaml 为了阻止CA删除正在运行的Pod，我们需要修改参数 cluster-autoscaler.kubernetes.io/safe-to-evict 的内容\nkubectl -n kube-system \\ annotate deployment.apps/cluster-autoscaler \\ cluster-autoscaler.kubernetes.io/safe-to-evict=\u0026quot;false\u0026quot; 接下来更新Cluster Autoscaler 的 Image 版本\n# we need to retrieve the latest docker image available for our EKS version export K8S_VERSION=$(kubectl version --short | grep 'Server Version:' | sed 's/[^0-9.]*\\([0-9.]*\\).*/\\1/' | cut -d. -f1,2) export AUTOSCALER_VERSION=$(curl -s \u0026quot;https://api.github.com/repos/kubernetes/autoscaler/releases\u0026quot; | grep '\u0026quot;tag_name\u0026quot;:' | sed -s 's/.*-\\([0-9][0-9\\.]*\\).*/\\1/' | grep -m1 ${K8S_VERSION}) echo $AUTOSCALER_VERSION kubectl -n kube-system \\ set image deployment.apps/cluster-autoscaler \\ cluster-autoscaler=us.gcr.io/k8s-artifacts-prod/autoscaling/cluster-autoscaler:v${AUTOSCALER_VERSION} 可以跟踪系统详细日志查看过程\nkubectl -n kube-system logs -f deployment/cluster-autoscaler 部署示例nginx应用  通过如下方式部署一个测试用的Nginx服务\ncd ~/environment/cluster-autoscaler/ cat \u0026lt;\u0026lt;EoF \u0026gt; nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 12 selector: matchLabels: app: nginx template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout 我们会发现部分pod会处于 “pending” 状态，因为它在等待EC2扩容\nkubectl get pods -l app=nginx -o wide --sort-by=.status.phase --watch 类似如下\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-to-scaleout-6fcd49fb84-dtvms 0/1 Pending 0 7s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-vq2k5 0/1 Pending 0 7s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-jg72q 0/1 Pending 0 7s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-vg4n2 0/1 Pending 0 7s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-6mm6r 0/1 Pending 0 7s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-khtvz 1/1 Running 0 8s 192.168.68.61 ip-192-168-115-52.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-p5sgq 1/1 Running 0 8s 192.168.94.122 ip-192-168-80-184.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-pwz8f 1/1 Running 0 8s 192.168.126.147 ip-192-168-115-52.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-rkhmb 1/1 Running 0 8s 192.168.20.36 ip-192-168-27-201.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-jr4hz 1/1 Running 0 8s 192.168.51.192 ip-192-168-27-201.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-fx4c4 1/1 Running 0 8s 192.168.104.159 ip-192-168-80-184.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-to-scaleout-6fcd49fb84-wlxnv 1/1 Running 0 8s 192.168.59.194 ip-192-168-27-201.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以查看详细的日志信息\nkubectl -n kube-system logs -f deployment/cluster-autoscaler 日志指出准备拉起2个节点\nI0625 15:42:01.840530 1 scale_up.go:364] Upcoming 2 nodes I0625 15:42:01.840624 1 scale_up.go:400] Skipping node group eks-2abd20a6-2880-5ca0-bdc4-71eb19df6135 - max size reached 此时我们可以打开 EC2的控制台，查看对应的EC2部署情况。\n也可以在kube-ops-view 里查看节点和Pod的即时变化： 再次查看eks集群的节点\nkubectl get nodes 我们会发现已经由原来的3个节点变成了5个节点，如下所示\nNAME STATUS ROLES AGE VERSION ip-192-168-115-52.ec2.internal Ready \u0026lt;none\u0026gt; 11h v1.20.4-eks-6b7464 ip-192-168-17-79.ec2.internal Ready \u0026lt;none\u0026gt; 5m57s v1.20.4-eks-6b7464 ip-192-168-27-201.ec2.internal Ready \u0026lt;none\u0026gt; 11h v1.20.4-eks-6b7464 ip-192-168-80-184.ec2.internal Ready \u0026lt;none\u0026gt; 11h v1.20.4-eks-6b7464 ip-192-168-82-53.ec2.internal Ready \u0026lt;none\u0026gt; 5m50s v1.20.4-eks-6b7464 推荐配置  在 EKS 的配置默认值里面，一个集群可以配置10个节点组，每个节点组可以有100个工作节点。\n我们通过eksctl创建集群时，没有指定节点组（默认1个），意味着这个集群最多可以扩容到100个EC2节点，然后再怎么加压也不会扩容了，这个时候就需要添加节点组（最简单的方式就是通过控制台手工增加）。\n"
},
{
	"uri": "/1-management/101-eks/1-eks-setup.html",
	"title": "A-部署EKS集群",
	"tags": [],
	"description": "",
	"content": "部署EKS集群  在本节中，您将通过 eksctl 工具创建EKS集群，并添加计算节点。\n安装 eksctl   进入 Cloud9 控制台 https://console.aws.amazon.com/cloud9/home?region=us-east-1 然后打开前面准备阶段创建的IDE环境\n  安装 eksctl 工具\n  curl --silent --location \u0026quot;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin eksctl version 配置自动完成  cat \u0026gt;\u0026gt; ~/.bash_profile \u0026lt;\u0026lt;EOF . \u0026lt;(eksctl completion bash) EOF source ~/.bash_profile 创建 EKS 集群   回到CloudFormation 控制台 https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks?filteringStatus=active\u0026amp;filteringText=\u0026amp;viewNested=true\u0026amp;hideStacks=false，找出准备阶段创建的堆栈输出   创建一个集群配置文件 ekscluster.yaml，注意用CloudFormation输出的值，替换VpcId等\n  这里是以 us-east-1 区域作为演示，如果您不是在 us-east-1 运行本实验，请注意替换 region\n --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ekslab region: us-east-1 version: \u0026quot;1.20\u0026quot; vpc: id: \u0026quot;\u0026lt;VpcId\u0026gt;\u0026quot; securityGroup: \u0026lt;ControlPlaneSecurityGroup\u0026gt; sharedNodeSecurityGroup: \u0026lt;SharedNodeSecurityGroup\u0026gt; subnets: public: us-east-1a: id: \u0026quot;\u0026lt;PublicSubnet01\u0026gt;\u0026quot; us-east-1b: id: \u0026quot;\u0026lt;PublicSubnet02\u0026gt;\u0026quot; private: us-east-1a: id: \u0026quot;\u0026lt;PrivateSubnet01\u0026gt;\u0026quot; us-east-1b: id: \u0026quot;\u0026lt;PrivateSubnet02\u0026gt;\u0026quot; cloudWatch: clusterLogging: enableTypes: [\u0026quot;*\u0026quot;] secretsEncryption: keyARN: \u0026lt;EKSKeyArn\u0026gt; 创建集群  eksctl create cluster -f ekscluster.yaml 确认集群 如果不报错的话，大约需要等待20分钟左右，期间可以去两个控制台查看进度：   cloudformation 的控制台 https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/ eks 的控制台 https://console.aws.amazon.com/eks/home?region=us-east-1#/clusters  在等待EKS集群创建的同时，可以先进行下一个模块 部署Kind集群\n创建完成后，查看集群状态\nkubectl version kubectl cluster-info 此时尚未添加任何计算节点  kubectl get no -o wide 这里为了让大家更好的理解EKS的工作原理，将创建集群和创建计算节点分开成2步。实际应用中，可以一步到位，创建集群的同时添加计算节点。\n 添加 Worker  创建一个nodegroup配置文件 eksng.yaml  --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ekslab region: us-east-1 managedNodeGroups: - name: managed-ng-default minSize: 1 maxSize: 3 desiredCapacity: 3 ssh: allow: true publicKeyName: container 这里没有指定具体的实例类型及例如磁盘配置等，默认使用m5.large类型，详细参数参见： https://eksctl.io/usage/schema/\n执行创建 nodegroup 命令  如果您在等待EKS集群创建完成的过程中，进入了下一个模块kind，请务必确保当前默认 context 是 ekslab\n eksctl create nodegroup -f eksng.yaml 不报错的情况下，等待约5分钟左右，创建完成  kubectl get no -o wide 成功搭建集群并添加计算节点后，可以继续下一个模块。\n"
},
{
	"uri": "/1-management/101-eks/2-kind-setup.html",
	"title": "B-部署Kind集群",
	"tags": [],
	"description": "",
	"content": "在本模块中，您将利用 Kind 工具，模拟自建一个k8s集群。\n本模块实践过程中，将手动创建一些 iptables 规则，如果您重启了Cloud9实例，则需要重新跑一遍相关命令\n 创建 Kind 集群   在Cloud9 IDE 中新开一个 terminal 窗口  安装 kind 工具  curl -sLo kind \u0026#34;https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64\u0026#34; sudo install -o root -g root -m 0755 kind /usr/local/bin/kind rm -f ./kind 配置 iptables 规则  echo \u0026#39;net.ipv4.conf.all.route_localnet = 1\u0026#39; | sudo tee /etc/sysctl.conf sudo sysctl -p /etc/sysctl.conf sudo iptables -t nat -A PREROUTING -p tcp -d 169.254.170.2 --dport 80 -j DNAT --to-destination 127.0.0.1:51679 sudo iptables -t nat -A OUTPUT -d 169.254.170.2 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679 准备集群配置文件  cat \u0026gt; kind.yaml \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.19.11@sha256:07db187ae84b4b7de440a73886f008cf903fcf5764ba8106a9fd5243d6f32729 extraPortMappings: - containerPort: 30000 hostPort: 30000 - containerPort: 30001 hostPort: 30001 EOF 创建本地集群  kind create cluster --config kind.yaml 查看 k8s context 等待约3分钟左右，kind 集群创建完毕  kubectl config get-contexts 以上截图 kind-kind 是默认Context。如果您是在上一步等待EKS集群创建过程中来到这一步，可能EKS集群创建尚未完成，您暂时只能看到一个Context。等EKS创建完成后，默认Context会变成ekslab\n 查看 Node  kubectl get no -o wide 切换默认Context Set the default context to the EKS cluster. 在本页面上，为方便操作，请确保默认Context是 kind-kind，如果不是，可以用以下命令设置  kubectl config use-context \u0026#34;\u0026lt;context-name\u0026gt;\u0026#34; 例如切换到 ekslab\nkubectl config use-context \u0026#34;i-010dcc656a3c7754c@ekslab.us-east-1.eksctl.io\u0026#34; 切回 kind-kind\nkubectl config use-context \u0026#34;kind-kind\u0026#34; 部署示例计数器应用   准备 counter-kind 目录并进入  mkdir counter-kind cd counter-kind 准备 postgres YAML部署文件   通过hostPath的方式创建 PersistentVolume 用来存储数据  cat \u0026gt; postgres.yaml \u0026lt;\u0026lt;EOF --- apiVersion: v1 kind: ConfigMap metadata: name: postgres-config labels: app: postgres data: POSTGRES_PASSWORD: supersecret init: | CREATE TABLE importantdata ( id int4 PRIMARY KEY, count int4 NOT NULL ); INSERT INTO importantdata (id , count) VALUES (1, 0); conf: | wal_level=logical wal_sender_timeout=0 listen_addresses=\u0026#39;*\u0026#39; --- kind: PersistentVolume apiVersion: v1 metadata: name: postgres-pv-volume labels: type: local app: postgres spec: storageClassName: manual capacity: storage: 5Gi accessModes: - ReadWriteMany hostPath: path: \u0026#34;/mnt/data\u0026#34; --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-pv-claim labels: app: postgres spec: storageClassName: manual accessModes: - ReadWriteMany resources: requests: storage: 5Gi --- apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: replicas: 1 serviceName: postgres selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: terminationGracePeriodSeconds: 5 containers: - name: postgres image: postgres:13 imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; ports: - containerPort: 5432 envFrom: - configMapRef: name: postgres-config volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb - mountPath: /docker-entrypoint-initdb.d name: init - name: postgresql-conf mountPath: /etc/postgresql args: [\u0026#34;-c\u0026#34;, \u0026#34;config_file=/etc/postgresql/postgresql.conf\u0026#34;] resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-pv-claim - name: init configMap: name: postgres-config items: - key: init path: init.sql - name: postgresql-conf configMap: name: postgres-config items: - key: conf path: postgresql.conf --- apiVersion: v1 kind: Service metadata: name: postgres labels: app: postgres spec: type: ClusterIP ports: - port: 5432 selector: app: postgres EOF 部署 postgres  kubectl --context kind-kind apply -f postgres.yaml 准备 counter 应用YAML部署文件  cat \u0026gt; counter.yaml \u0026lt;\u0026lt;EOF --- apiVersion: apps/v1 kind: Deployment metadata: name: counter labels: app: counter spec: replicas: 2 selector: matchLabels: app: counter template: metadata: labels: app: counter spec: containers: - name: counter image: public.ecr.aws/jg/counter:latest ports: - containerPort: 8000 resources: requests: memory: \u0026#34;16Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; --- apiVersion: v1 kind: Service metadata: name: counter-service spec: type: NodePort selector: app: counter ports: - port: 8000 name: http nodePort: 30000 EOF 部署 counter  kubectl --context kind-kind apply -f counter.yaml 确认部署   确保应用和数据库都是正常 running 状态  kubectl --context kind-kind get pods 发布示例计数器应用   进入VPC控制台“安全组”页面 https://console.aws.amazon.com/vpc/home?region=us-east-1#securityGroups:   搜索 ExternalSecurityGroup 记录安全组 Id 和 VPC Id  注意替换 VpcId 和 ExternalSecurityGroupId\nexport VPC_ID=vpc-05776f9487841de73 export EXTERNAL_SECURITY_GROUP=sg-091de3108e6ee6673 export C9_INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id) 创建ALB   与Cloud9 实例在同一个子网，并绑定一样的安全组 ExternalSecurityGroup  export ALB_ARN=$(aws elbv2 create-load-balancer \\  --name counter \\  --subnets $(aws ec2 describe-subnets \\  --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; \u0026#34;Name=tag:kubernetes.io/role/elb,Values=1\u0026#34; \\  --query \u0026#39;Subnets[*].SubnetId\u0026#39; \\  --output text) \\  --type application --security-groups $EXTERNAL_SECURITY_GROUP \\  --query \u0026#39;LoadBalancers[0].LoadBalancerArn\u0026#39; \\  --output text) 创建目标群组  export TG_ARN=$(aws elbv2 create-target-group \\  --name counter-target --protocol HTTP \\  --port 30000 --target-type instance \\  --vpc-id ${VPC_ID} --query \u0026#39;TargetGroups[0].TargetGroupArn\u0026#39; \\  --output text) 注册实例到目标群组  aws elbv2 register-targets \\  --target-group-arn ${TG_ARN} \\  --targets Id=${C9_INSTANCE_ID} 创建默认侦听器  aws elbv2 wait load-balancer-available \\  --load-balancer-arns $ALB_ARN \\  \u0026amp;\u0026amp; export ALB_LISTENER=$(aws elbv2 create-listener \\  --load-balancer-arn ${ALB_ARN} \\  --port 80 --protocol HTTP \\  --default-actions Type=forward,TargetGroupArn=${TG_ARN} \\  --query \u0026#39;Listeners[0].ListenerArn\u0026#39; \\  --output text) 需要等待大约2分钟（依赖ALB 创建完成，状态变为可用）\n至此如果没有报错，我们的应用已经成功对外发布   可以通过命令查看ALB 地址  echo \u0026#34;http://\u0026#34;$(aws elbv2 describe-load-balancers \\  --load-balancer-arns $ALB_ARN \\  --query \u0026#39;LoadBalancers[0].DNSName\u0026#39; --output text)  或者通过ELB控制台查看 https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LoadBalancers:sort=loadBalancerName   在浏览器中打开ALB地址，点击按钮count++，产生计数  "
},
{
	"uri": "/1-management/102-exposing/1-ingress-lbc.html",
	"title": "A-Ingress",
	"tags": ["Service", "Expose"],
	"description": "",
	"content": "介绍  本章节我们演示如何通过Ingress暴露服务给外部终端用户访问。\nIngress 是控制前端路由进来到服务的组件。不同于其他的 kube-controller-manager 控制器，Ingress Controller 非系统自带，不会自动安装启动，需要自行安装配置。\n在EKS里面，默认的 Ingress 是 AWS Load Balancer Controller 更多细节见这里\nAWS ALB Ingress Controller 已经更名为 AWS Load Balancer Controller.\n 针对不同的服务类型，选用不同的组件\n Ingress： 使用 ALB（Application Load Balancers） Service： 使用 NLB（Network Load Balancers）  ALB 目前支持的功能:\n host or path based routing TLS (Transport Layer Security) termination, WebSockets HTTP/2 AWS WAF (Web Application Firewall) integration integrated access logs, and health checks  部署 AWS Load Balancer Controller  参考 https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.2/deploy/installation/\n 检查 helm  helm version --short  此处我们使用 helm 来安装和配置 ALB Controller，如果还没安装 helm，请参考 002-准备管理环境\n 关联 IAM OIDC provider  eksctl utils associate-iam-oidc-provider \\  --region=$AWS_REGION \\  --cluster=ekslab \\  --approve  更多信息参考Amazon EKS 文档 IAM Roles for Service Accounts\n 创建 IAM policy AWSLoadBalancerControllerIAMPolicy  curl -o lbc_iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json aws iam create-policy \\  --policy-name AWSLoadBalancerControllerIAMPolicy \\  --policy-document file://lbc_iam_policy.json 创建IAM角色 和 ServiceAccount  eksctl create iamserviceaccount \\ --cluster=ekslab \\ --namespace=kube-system \\ --name=aws-load-balancer-controller \\ --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy \\ --override-existing-serviceaccounts \\ --approve 创建TargetGroupBinding CRDS  kubectl apply -k \u0026quot;github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\u0026quot; kubectl get crd 通过 Helm 部署 AWS Load Balancer Controller  helm repo add eks https://aws.github.io/eks-charts helm upgrade -i aws-load-balancer-controller \\  eks/aws-load-balancer-controller \\  -n kube-system \\  --set clusterName=ekslab \\  --set serviceAccount.create=false \\  --set serviceAccount.name=aws-load-balancer-controller kubectl -n kube-system rollout status deployment aws-load-balancer-controller  上面命令默认安装的是最新版本，如果需要安装指定版本，可以通过 helm search aws-load-balancer-controller 命令搜索可用版本\n 部署样例应用  下面部署示例应用 2048 game\n 进入Cloud9控制台，准备工作目录  mkdir ${HOME}/environment/2048 cd ${HOME}/environment/2048 准备部署 YAML 文件  cat \u0026gt; 2048_application.yaml \u0026lt;\u0026lt;EOF --- apiVersion: v1 kind: Namespace metadata: name: game-2048 --- apiVersion: apps/v1 kind: Deployment metadata: namespace: game-2048 name: deployment-2048 spec: selector: matchLabels: app.kubernetes.io/name: app-2048 replicas: 5 template: metadata: labels: app.kubernetes.io/name: app-2048 spec: containers: - image: alexwhen/docker-2048 imagePullPolicy: Always name: app-2048 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: namespace: game-2048 name: service-2048 spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: app.kubernetes.io/name: app-2048 EOF cat \u0026gt; 2048_ingress.yaml \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: game-2048 name: ingress-2048 annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: service-2048 port: number: 80 EOF 执行部署  kubectl apply -f . 等待部署完成后，查看 Ingress  kubectl get ingress/ingress-2048 -n game-2048  ALB 部署可用需要等待约2-3分钟\n 结果类似下图： 获取更详细的 Ingress 信息  export GAME_INGRESS_NAME=$(kubectl -n game-2048 get targetgroupbindings -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) kubectl -n game-2048 get targetgroupbindings ${GAME_INGRESS_NAME} -o yaml 查看URL地址  export GAME_2048=$(kubectl get ingress/ingress-2048 -n game-2048 -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo http://${GAME_2048} 浏览器中查看游戏  "
},
{
	"uri": "/1-management/103-dns/1-external-dns.html",
	"title": "A-ExternalDNS",
	"tags": ["Service", "Expose"],
	"description": "",
	"content": "介绍  ExternalDNS 使 Kubernetes 资源可以通过公共 DNS 服务器发现。它从 Kubernetes API 中检索资源（服务、入口等）列表，以确定所需的 DNS 记录列表。它本身不是 DNS 服务器，而只是相应地配置其他 DNS 提供商，例如 AWS Route 53。\n官网 https://github.com/kubernetes-sigs/external-dns\n前置条件  一个测试域名 已经完成 102-服务暴露-A-Ingress ，AWS Load Balancer Controller已经安装  部署 ExternalDNS  参考 https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/alb-ingress.md\n 进入Cloud9 IDE，准备目录和域名变量 请替换 bwcx.tk 为您自己的域名  mkdir ${HOME}/environment/dns cd ${HOME}/environment/dns export DNS_TEST_DOMAIN=\u0026quot;bwcx.tk\u0026quot; 创建 策略  cat \u0026gt; iam-dns-policy.json \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;route53:ChangeResourceRecordSets\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:route53:::hostedzone/*\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;route53:ListHostedZones\u0026quot;, \u0026quot;route53:ListResourceRecordSets\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;*\u0026quot; ] } ] } EOF aws iam create-policy \\ --policy-name AllowExternalDNSUpdates \\ --policy-document file://iam-dns-policy.json 创建 Service Account  eksctl create iamserviceaccount \\ --name external-dns \\ --namespace kube-system \\ --cluster ekslab \\ --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/AllowExternalDNSUpdates \\ --override-existing-serviceaccounts \\ --approve 创建完成以后检查\nkubectl describe sa external-dns -n kube-system 检查 RBAC是否启用（官方文档里根据RBAC是否启用，部署文件有差异）  kubectl api-versions | grep rbac.authorization.k8s.io  这里实验默认是启用的，输出类似如下：\n rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 创建 Route 53 托管区域  aws route53 create-hosted-zone --name ${DNS_TEST_DOMAIN}\u0026#34;.\u0026#34; --caller-reference \u0026#34;external-dns-test-$(date +%s)\u0026#34; 创建完成后，可以在 Route 53 控制台查看 https://console.aws.amazon.com/route53/v2/hostedzones#\n如果您的域名不是通过AWS注册的，需要配置类似下图的 name servers 地址到您的DNS解析提供商\n 记录托管区ID  aws route53 list-hosted-zones-by-name --output json --dns-name ${DNS_TEST_DOMAIN}\u0026quot;.\u0026quot; | jq -r '.HostedZones[0].Id' 准备部署文件  cat \u0026gt; external-dns.yaml \u0026lt;\u0026lt;EOF apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: external-dns rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;services\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;list\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: external-dns-viewer roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: external-dns subjects: - kind: ServiceAccount name: external-dns namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: external-dns namespace: kube-system spec: selector: matchLabels: app: external-dns strategy: type: Recreate template: metadata: labels: app: external-dns spec: serviceAccountName: external-dns securityContext: fsGroup: 65534 containers: - name: external-dns image: k8s.gcr.io/external-dns/external-dns:v0.8.0 args: - --source=service - --source=ingress - --domain-filter=${DNS_TEST_DOMAIN} # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=aws - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both) - --registry=txt - --txt-owner-id=my-identifier EOF 执行部署并检查  kubectl apply -f external-dns.yaml 检查\nkubectl get pod -l app=external-dns -n kube-system kubectl logs -f $(kubectl get po -n kube-system | egrep -o 'external-dns[A-Za-z0-9-]+') -n kube-system 部署样例应用  下面更新示例应用 2048 game\n 进入 2048 目录  cd ${HOME}/environment/2048 创建新 ingress 文件  cat \u0026gt; 2048_ingress_dns.yaml \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: game-2048 name: ingress-2048 annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # for creating record-set external-dns.alpha.kubernetes.io/hostname: 2048.${DNS_TEST_DOMAIN} # give your domain name here external-dns.alpha.kubernetes.io/ttl: \u0026#34;60\u0026#34; spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: service-2048 port: number: 80 EOF 执行部署  kubectl apply -f 2048_ingress_dns.yaml 查看 ExternalDNS 日志  kubectl logs -f $(kubectl get po -n kube-system | egrep -o \u0026#39;external-dns[A-Za-z0-9-]+\u0026#39;) -n kube-system 输出类似如下：\ntime=\u0026quot;2021-06-25T09:10:51Z\u0026quot; level=info msg=\u0026quot;Desired change: CREATE 2048.bwcx.tk A [Id: /hostedzone/Z0759090APIKX508GBYY]\u0026quot; time=\u0026quot;2021-06-25T09:10:51Z\u0026quot; level=info msg=\u0026quot;Desired change: CREATE 2048.bwcx.tk TXT [Id: /hostedzone/Z0759090APIKX508GBYY]\u0026quot; time=\u0026quot;2021-06-25T09:10:51Z\u0026quot; level=info msg=\u0026quot;2 record(s) in zone bwcx.tk. [Id: /hostedzone/Z0759090APIKX508GBYY] were successfully updated\u0026quot; 打开 Route53 控制台 https://console.aws.amazon.com/route53/v2/hostedzones#  在明细里可以看到自动添加的DNS解析记录 浏览器中检查  "
},
{
	"uri": "/1-management/101-eks/3-velero-app.html",
	"title": "C-使用Velero迁移应用",
	"tags": [],
	"description": "",
	"content": "Velero（以前称为Heptio Ark）是一个开源工具，可以安全地备份和还原，执行灾难恢复以及迁移Kubernetes 集群资源和持久卷. 用于：\n 备份集群并在丢失的情况下进行还原 将集群资源迁移到其他集群 将生产集群复制到开发和测试集群  前置准备   打开 Cloud9 IDE，确保在environment目录下  cd ~/environment 创建用于存储集群备份数据的 S3 桶  美东区域（us-east-1），请用如下命令\n export VELERO_BUCKET=$(aws s3api create-bucket \\ --bucket ekslab-backup-$(date +%s)-$RANDOM \\ --region $AWS_REGION \\ --| jq -r '.Location' \\ --| tr -d /)  如果不是美东区域，请用如下命令\n export VELERO_BUCKET=$(aws s3api create-bucket \\ --bucket ekslab-backup-$(date +%s)-$RANDOM \\ --region $AWS_REGION \\ --create-bucket-configuration LocationConstraint=$AWS_REGION \\ --| jq -r '.Location' \\ --| cut -d'/' -f3 \\ --| cut -d'.' -f1) 设置 VELERO_BUCKET 环境变量  echo \u0026quot;export VELERO_BUCKET=${VELERO_BUCKET}\u0026quot; | tee -a ~/.bash_profile 为Velero准备一个IAM用户  aws iam create-user --user-name velero 附加需要的权限   创建权限策略  cat \u0026gt; velero-policy.json \u0026lt;\u0026lt;EOF { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;ec2:DescribeVolumes\u0026quot;, \u0026quot;ec2:DescribeSnapshots\u0026quot;, \u0026quot;ec2:CreateTags\u0026quot;, \u0026quot;ec2:CreateVolume\u0026quot;, \u0026quot;ec2:CreateSnapshot\u0026quot;, \u0026quot;ec2:DeleteSnapshot\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:AbortMultipartUpload\u0026quot;, \u0026quot;s3:ListMultipartUploadParts\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::${VELERO_BUCKET}/*\u0026quot; ] }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:ListBucket\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::${VELERO_BUCKET}\u0026quot; ] } ] } EOF  关联策略到 velero 用户  aws iam put-user-policy \\ --user-name velero \\ --policy-name velero \\ --policy-document file://velero-policy.json 创建AKSK  aws iam create-access-key --user-name velero \u0026gt; velero-access-key.json 确认AKSK正确创建  cat velero-access-key.json 输出结果类似如下，里面会有生成的AccessKeyId和SecretAccessKey：\n{ \u0026quot;AccessKey\u0026quot;: { \u0026quot;UserName\u0026quot;: \u0026quot;velero\u0026quot;, \u0026quot;Status\u0026quot;: \u0026quot;Active\u0026quot;, \u0026quot;CreateDate\u0026quot;: \u0026quot;2021-06-22T07:36:35+00:00\u0026quot;, \u0026quot;SecretAccessKey\u0026quot;: \u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt;, \u0026quot;AccessKeyId\u0026quot;: \u0026lt;AWS_ACCESS_KEY_ID\u0026gt; } } 准备 credentials 文件   在 (~/environment) 目录下，创建如下文件  export VELERO_ACCESS_KEY_ID=$(cat velero-access-key.json | jq -r '.AccessKey.AccessKeyId') export VELERO_SECRET_ACCESS_KEY=$(cat velero-access-key.json | jq -r '.AccessKey.SecretAccessKey') cat \u0026gt; velero-credentials \u0026lt;\u0026lt;EOF [default] aws_access_key_id=$VELERO_ACCESS_KEY_ID aws_secret_access_key=$VELERO_SECRET_ACCESS_KEY EOF 安装 Velero Cli  参考 https://github.com/vmware-tanzu/velero-plugin-for-aws\n 下载客户端工具   下载当前最新版本 latest release\u0026rsquo;s 例如 1.6.0  wget https://github.com/vmware-tanzu/velero/releases/download/v1.6.0/velero-v1.6.0-linux-amd64.tar.gz 解压安装并检查  tar -xvf velero-v1.6.0-linux-amd64.tar.gz -C /tmp sudo mv /tmp/velero-v1.6.0-linux-amd64/velero /usr/local/bin velero version 当前还没有安装服务端，提示 Get Server Version 错误是正常的\n部署 Velero Server 到 Kind  首先确认当前 context 为 kind-kind  kubectl config current-context 如果不是，请用下面命令切换\nkubectl config use-context kind-kind 安装 Velero Server 端  velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.2.0 \\ --bucket $VELERO_BUCKET \\ --backup-location-config region=$AWS_REGION \\ --snapshot-location-config region=$AWS_REGION \\ --secret-file ./velero-credentials 检查部署  Velero 资源会部署到专门的namespace：“velero”\nkubectl --context kind-kind get all -n velero 编辑 deployment   找出 cluster name  kubectl config view --minify -o jsonpath='{.clusters[].name}' 类似输出 kind-kind\n添加到velero部署环境变量，添加 AWS_CLUSTER_NAME 到 spec.template.spec.env 下\nkubectl --context kind-kind edit deploy/velero -n velero 再次检查部署  kubectl --context kind-kind describe deployment velero -n velero 部署 Velero Server 到 EKS  切换到 eks 集群 context  kubectl config use-context \u0026quot;i-010dcc656a3c7754c@ekslab.us-east-1.eksctl.io\u0026quot; 安装 Velero Server 端  velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.2.0 \\ --bucket $VELERO_BUCKET \\ --backup-location-config region=$AWS_REGION \\ --snapshot-location-config region=$AWS_REGION \\ --secret-file ./velero-credentials 检查 velero namesapce  kubectl get all -n velero 编辑 deployment   找出 cluster name  kubectl config view --minify -o jsonpath='{.clusters[].name}' 类似输出 ekslab.us-east-1.eksctl.io\n添加到velero部署环境变量，添加 AWS_CLUSTER_NAME 到 spec.template.spec.env 下\nkubectl edit deploy/velero -n velero 再次检查 velero namesapce 资源，确保仍然正常运行  kubectl get all -n velero 检查 default namespace  kubectl get pods 目前 default namespace 尚未部署任何资源\n备份迁移 参考 https://velero.io/docs/v1.6/migration-case/\n备份集群 1（kind）  首先切换到 kind context  kubectl config use-context \u0026quot;kind-kind\u0026quot; 备份 default namespace  velero backup create kind-default-backup --include-namespaces default 检查备份状态  velero backup describe kind-default-backup 确保处于 Completed 状态 如果发现问题，可以通过以下命令查看日志\nvelero backup logs kind-default-backup 进入 S3 控制台 https://s3.console.aws.amazon.com/s3/home?region=us-east-1，查看是否有备份文件生成  恢复到集群 2（eks）  首先切换到 eks context 注意替换 context 名字  kubectl config use-context \u0026quot;i-010dcc656a3c7754c@ekslab.us-east-1.eksctl.io\u0026quot; 检查是否能看到集群1（kind）的备份  velero backup get 查看备份详细信息  velero backup describe kind-default-backup 恢复 kind default namespace 到 eks default namespace  velero restore create --from-backup kind-default-backup 检查恢复状态   查看恢复任务列表  velero restore get 查看任务明细\nvelero restore describe kind-default-backup-20210622095748 确保处于 Completed 状态，如果有问题，可以查看日志\nvelero restore logs kind-default-backup-20210622095748 检查部署情况  kubectl get all 创建 counter-eks 目录（在 ~/environment 下）  cd ~/environment mkdir counter-eks cd counter-eks 更新 counter-service 服务配置   导出当前 counter-service YAML  kubectl get svc counter-service -o yaml \u0026gt; counter-service.yaml  利用EKS集成的 LoadBalancer 快速发布到公网  编辑文件：\nvi counter-service.yaml  将 type: NodePort 改成 type: LoadBalancer 将 port: 8000 改为 port: 80 删除 nodePort: 30000  类似如下： 然后再应用更新\nkubectl apply -f counter-service.yaml 查看更新后的服务  kubectl get svc 在浏览器访问服务   获取地址  echo \u0026quot;http://\u0026quot;$(kubectl get svc counter-service --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') 这里 Count: 0 符合预期，将在后续模块迁移数据。参考官方文档： Note that Velero does not natively support the migration of persistent volumes snapshots across cloud providers. https://velero.io/docs/v1.6/migration-case/\n "
},
{
	"uri": "/1-management/101-eks/4-csi-setup.html",
	"title": "D-(可选)设置CSI存储",
	"tags": [],
	"description": "",
	"content": "检查数据    打开 Cloud9 IDE\n  首先查看 kind 集群 postgres 数据\n  kubectl --context kind-kind exec -ti postgres-0 -- psql -U postgres 查询计数器\nselect * from importantdata; 可以看到当前已经点击了 12 次\n然后检查 eks 集群 postgres 数据 请先确保默认 context 是 ekslab  kubectl exec -ti postgres-0 -- psql -U postgres 查询计数器\nselect * from importantdata; 可以看到当前计数为 0\n检查存储   回顾 kind 集群 pv, pvc, storageclass  kubectl --context kind-kind get pv,pvc,sc 查看 eks 集群 pv, pvc, storageclass  kubectl get pv,pvc,sc 可以看到 velero 针对 aws 自动转换了 storageclass\n查看 storageclass 明细  kubectl describe sc gp2 当前 velero 版本还是采用的 In-tree EBS plugin   In-tree EBS plugin https://docs.aws.amazon.com/zh_cn/eks/latest/userguide/storage-classes.html 2021-05月，Amazon EBS已经支持 CSI 参考 https://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ebs-container-storage-interface-driver-is-now-generally-available/  构建新 Amazon EBS CSI  参考 https://github.com/kubernetes-sigs/aws-ebs-csi-driver\n 配置 IAM Policy  export EBS_CSI_POLICY_NAME=\u0026#34;Amazon_EBS_CSI_Driver\u0026#34; mkdir ${HOME}/environment/ebs_statefulset cd ${HOME}/environment/ebs_statefulset # download the IAM policy document curl -sSL -o ebs-csi-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-ebs-csi-driver/master/docs/example-iam-policy.json # Create the IAM policy aws iam create-policy \\  --region ${AWS_REGION} \\  --policy-name ${EBS_CSI_POLICY_NAME} \\  --policy-document file://${HOME}/environment/ebs_statefulset/ebs-csi-policy.json # export the policy ARN as a variable export EBS_CSI_POLICY_ARN=$(aws --region ${AWS_REGION} iam list-policies --query \u0026#39;Policies[?PolicyName==`\u0026#39;$EBS_CSI_POLICY_NAME\u0026#39;`].Arn\u0026#39; --output text) 为Service Account配置 IAM Role   通过 eksctl 工具来关联身份提供商和创建SA  # Create an IAM OIDC provider for your cluster eksctl utils associate-iam-oidc-provider \\  --region=$AWS_REGION \\  --cluster=ekslab \\  --approve # Create a service account eksctl create iamserviceaccount \\  --cluster ekslab \\  --name ebs-csi-controller-irsa \\  --namespace kube-system \\  --attach-policy-arn $EBS_CSI_POLICY_ARN \\  --override-existing-serviceaccounts \\  --approve 检查身份关联，打开IAM控制台 https://console.aws.amazon.com/iamv2/home#/identity_providers  部署 Amazon EBS CSI Driver  Deploy the Amazon EBS CSI Driver # add the aws-ebs-csi-driver as a helm repo helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver helm repo update # search for the driver helm search repo aws-ebs-csi-driver 当前最新driver版本为1.1.0，helm chart 版本为1.2.3 helm upgrade --install aws-ebs-csi-driver \\  --version=1.2.3 \\  --namespace kube-system \\  --set serviceAccount.controller.create=false \\  --set serviceAccount.snapshot.create=false \\  --set enableVolumeScheduling=true \\  --set enableVolumeResizing=true \\  --set enableVolumeSnapshot=true \\  --set serviceAccount.snapshot.name=ebs-csi-controller-irsa \\  --set serviceAccount.controller.name=ebs-csi-controller-irsa \\  --set Value.node.tolerateAllTaints=true \\  aws-ebs-csi-driver/aws-ebs-csi-driver 检查部署  helm ls -n kube-system kubectl -n kube-system rollout status deployment ebs-csi-controller kubectl get pod -n kube-system -l \u0026quot;app.kubernetes.io/name=aws-ebs-csi-driver,app.kubernetes.io/instance=aws-ebs-csi-driver\u0026quot; 创建新 Storage Class   准备 storageclass 配置文件  cat \u0026lt;\u0026lt; EoF \u0026gt; ${HOME}/environment/ebs_statefulset/csi-storageclass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: csi-gp2 provisioner: ebs.csi.aws.com # Amazon EBS CSI driver parameters: type: gp2 encrypted: 'true' # EBS volumes will always be encrypted by default volumeBindingMode: WaitForFirstConsumer # EBS volumes are AZ specific reclaimPolicy: Delete mountOptions: - debug EoF  执行命令  kubectl apply -f ${HOME}/environment/ebs_statefulset/csi-storageclass.yaml 设置为默认 storageclass  kubectl patch storageclass csi-gp2 -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;:\u0026quot;true\u0026quot;}}}' 检查新创建的 storageclass  kubectl describe storageclass csi-gp2 重新部署 postgres   在浏览器打开新计数器页面，点击一次（方便后续对比）  准备新的 pvc  cd ${HOME}/environment/counter-eks cat \u0026gt; postgres-pv-claim-new.yaml \u0026lt;\u0026lt;EOF kind: PersistentVolumeClaim apiVersion: v1 metadata: name: postgres-pv-claim-new labels: app: postgres spec: storageClassName: csi-gp2 accessModes: - ReadWriteOnce resources: requests: storage: 5Gi EOF 准备新 postgres statefulset 文件  cat \u0026gt; postgres-statefulset-new.yaml \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: StatefulSet metadata: name: postgres spec: replicas: 1 serviceName: postgres selector: matchLabels: app: postgres template: metadata: labels: app: postgres spec: terminationGracePeriodSeconds: 5 containers: - name: postgres image: postgres:13 imagePullPolicy: \u0026quot;IfNotPresent\u0026quot; ports: - containerPort: 5432 envFrom: - configMapRef: name: postgres-config volumeMounts: - mountPath: /var/lib/postgresql/data name: postgredb subPath: postgres - mountPath: /docker-entrypoint-initdb.d name: init resources: requests: memory: \u0026quot;64Mi\u0026quot; cpu: \u0026quot;250m\u0026quot; limits: memory: \u0026quot;128Mi\u0026quot; cpu: \u0026quot;500m\u0026quot; volumes: - name: postgredb persistentVolumeClaim: claimName: postgres-pv-claim-new - name: init configMap: name: postgres-config items: - key: init path: init.sql EOF 导出当前 postgres statefulset YAML文件，然后删除  kubectl get statefulset postgres -o yaml \u0026gt; postgres-statefulset.yaml 执行删除\nkubectl delete -f postgres-statefulset.yaml 重新部署 postgres statefulset  kubectl apply -f postgres-pv-claim-new.yaml kubectl apply -f postgres-statefulset-new.yaml 确认部署  kubectl get po 确保都处于 running 状态 刷新浏览器页面  重置存储后，计数器复归为0\n"
},
{
	"uri": "/1-management/101-eks/5-dms-data.html",
	"title": "E-使用DMS迁移数据",
	"tags": [],
	"description": "",
	"content": "AWS Database Migration Service (AWS DMS) 是一项云服务，可轻松迁移关系数据库、数据仓库、NoSQL 数据库及其他类型的数据存储。您可以使用。AWSDMS 将您的数据迁移到AWS云，在本地实例之间（通过AWS云设置）或云与本地设置的组合之间。\n配置数据库内网访问    打开 Cloud9 IDE\n  首先配置 kind 集群 postgres\n   准备 NodePort 服务配置文件  cd ${HOME}/environment/counter-kind cat \u0026gt; postgres-nodeport-svc.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: postgres-nodeport labels: app: postgres-nodeport spec: type: NodePort ports: - port: 5432 nodePort: 30001 selector: app: postgres EOF 创建新NodePort服务  kubectl --context kind-kind apply -f postgres-nodeport-svc.yaml 检查连接   安装 postgresql 客户端  sudo yum install postgresql -y 尝试连接\npsql -h localhost -p 30001 -U postgres 输入密码 supersecret\n检查数据  select * from importantdata; 请确保本地能够连接到 kind 集群里的 postgresql，并能成功查询数据后再进入下一步\n 然后配置 eks 集群 postgres   请先确保当前默认 context 是 ekslab，如果不是请先进行切换  kubectl config current-context 同样先准备 NodePort 服务配置文件  cd ${HOME}/environment/counter-eks cat \u0026gt; postgres-nodeport-svc.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: postgres-nodeport labels: app: postgres-nodeport spec: type: NodePort ports: - port: 5432 nodePort: 30001 selector: app: postgres EOF  这里为了实验方便，手动指定了NodePort 端口为 30001，如果出现端口号被占用，可以指定其他空闲端口号，或者不指定（部署自动生成以后再找出端口号）\n 执行部署  kubectl apply -f postgres-nodeport-svc.yaml 检查连接和数据   首先找出 postgres 所在的节点  kubectl get po postgres-0 -o wide  然后连接到 eks postgres  psql -h ip-192-168-91-9.ec2.internal -p 30001 -U postgres 输入密码 supersecret\n 然后查询数据  select * from importantdata; 请确保本地能够连接到 eks 集群里的 postgresql，并能成功查询数据后再进入下一步\n 部署DMS    进入 DMS 控制台“子网组”页面 https://console.aws.amazon.com/dms/v2/home?region=us-east-1#subnetGroup\n  点击 创建子网组，填写配置参数\n   名称：eks-dms-subnet-group 描述：subnet group for eks lab VPC：选择准备阶段创建的labs-eks-VPC 添加子网：选择2个私有子网   填完参数后，点击 创建子网组\n  进入 DMS 控制台“复制实例”页面 https://console.aws.amazon.com/dms/v2/home?region=us-east-1#replicationInstances\n  点击 创建复制实例，填写配置参数\n   名称：postgres-kind-2-eks VPC：选择准备阶段创建的labs-eks-VPC并取消“公开访问”  展开高级安全和网络配置，选择前面创建的子网组，并添加2个安全组 ControlPlaneSecurityGroup 和 SharedNodeSecurityGroup   填完参数后，点击 创建\n  进入 DMS 控制台“终端节点”页面 https://console.aws.amazon.com/dms/v2/home?region=us-east-1#endpointList\n  点击 创建终端节点，选择 源终端节点，准备创建数据复制来源（kind）连接\n  填写来源终端参数后，点击创建终端节点\n   终端节点标识符：postgres-kind 源引擎：PostgreSQL 手动提供访问信息：  服务器名称 Cloud9 实例的内网IP，可以通过控制台查看，或用命令 curl http://169.254.169.254/latest/meta-data/local-ipv4，例如 192.168.62.38 端口 30001 用户名 postgres 密码 supersecret 数据库名称 postgres     再次点击 创建终端节点，选择 目标终端节点，准备创建数据复制目标（eks）连接\n  填写目标终端参数后，点击创建终端节点\n   终端节点标识符：postgres-eks 目标引擎：PostgreSQL 手动提供访问信息：  服务器名称是运行 postgres pod 的工作节点内网IP (注意是IP，不是DNS)，例如 192.168.91.9，可以通过命令查找 kubectl describe po postgres-0 | grep Node  端口 30001 用户名 postgres 密码 supersecret 数据库名称 postgres    测试连接   首先请确保复制实例 postgres-kind-2-eks 状态为 可用   两个终端节点状态也都为 可用   首先验证源连接，点击 postgres-kind，选择连接 tab 页，再点击测试连接   选择之前创建的复制实例 postgres-kind-2-eks，然后运行测试，确保状态为 successful   重复步骤，选择 postgres-eks 终端，运行测试，确保状态为 successful  请确保2个终端节点连接测试，状态都为successful才进入下一步\n 迁移数据    进入 DMS 控制台“数据库迁移任务”页面 https://console.aws.amazon.com/dms/v2/home?region=us-east-1#tasks\n  点击创建任务，配置任务参数\n   任务标识符：migrate-2-eks 复制实例：postgres-kind-2-eks 源数据库终端节点：postgres-kind 目标数据库终端节点：postgres-eks 迁移类型：迁移现有数据并复制正在进行的更改  任务设置里勾选**启用 CloudWatch 日志  配置复制规则，展开“表映像”-\u0026gt;“选择规则”，点击“添加新选择规则”，勾选“输入架构”   其他参数保持默认，点击创建任务\n  检查任务执行情况\n  当进度为100%时，进入下一步。\n如果出现问题，可以进入详情页，然后查看日志\n浏览器打开EKS ELB 地址  可以看到此时原来在Kind集群的计数统计已经同步过来\nCDC 同步测试，因为我们在前面的步骤中，复制任务模式是完全加载，持续复制，kind源库的增量数据也会同步到eks目标库   打开两个浏览器窗口，左边是 kind 集群的服务，右边是eks 集群的服务 在左边点击count++ 按钮，然后在右边点击浏览器刷新按钮  可以看到增量数据已经由DMS自动同步过去\n本 workshop 仅为演示方便，因而前面的步骤直接将迁移到eks的服务发布到外网，方便查看计数。 真实场景下，应是测试验证OK后，选择迁移窗口，应用和数据都迁移完成了，再停掉老集群，切换新集群对外提供服务。  结语  恭喜！ 您已经完成了 Workshop。\n"
},
{
	"uri": "/1-management/105-security/1-acm.html",
	"title": "A-ACM",
	"tags": ["Security"],
	"description": "",
	"content": "前置条件  一个测试域名 已经完成 103-DNS集成-A-ExternalDNS ，Route 53 托管区域已经配置  准备证书   进入Cloud9 IDE，配置域名变量 请替换 bwcx.tk 为您自己的域名  export DNS_TEST_DOMAIN=\u0026quot;bwcx.tk\u0026quot;  打开 ACM 控制台 https://console.aws.amazon.com/acm/home?region=us-east-1#/ 依次选择 “预置证书” -\u0026gt; “请求公有证书”\n  在添加域名页面，*.bwcx.tk 填写测试域名\n   验证方法选择 DNS 验证，后面步骤直接跳过，到最后一步点击确认并请求\n  在验证页面，展开配置项，点击在Route 53中创建记录\n  将自动在Route 53 中创建记录 等待1～2分钟左右，自动验证完成，状态变为已颁发  查看详细信息，记录 ARN  将 ARN 配置到变量，方便后续使用  export ACM_TEST_ARN=\u0026lt;ACM ARN\u0026gt; 部署样例应用  下面更新示例应用 2048 game\n 进入 2048 目录  cd ${HOME}/environment/2048 创建新 ingress 文件  cat \u0026gt; 2048_ingress_dns_acm.yaml \u0026lt;\u0026lt;EOF apiVersion: networking.k8s.io/v1 kind: Ingress metadata: namespace: game-2048 name: ingress-2048 annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: instance # for creating record-set external-dns.alpha.kubernetes.io/hostname: 2048.${DNS_TEST_DOMAIN} # give your domain name here external-dns.alpha.kubernetes.io/ttl: \u0026#34;60\u0026#34; alb.ingress.kubernetes.io/certificate-arn: ${ACM_TEST_ARN} alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;:443}]\u0026#39; spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: service-2048 port: number: 80 EOF 执行部署  kubectl apply -f 2048_ingress_dns_acm.yaml 检查部署，打开EC2 ELB控制台 https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#LoadBalancers:sort=loadBalancerName  浏览器中检查 https://2048.bwcx.tk/  "
},
{
	"uri": "/about.html",
	"title": "关于我们",
	"tags": [],
	"description": "",
	"content": " 警告：本站所有技术文章和截图，仅供读者作为学习参考资料，严禁用于违法犯罪活动，否则产生的一切后果由读者自行承担，本站和所有作者均不承担任何法律及连带责任！\n   作者： Weiqiong Chen, Robin Long 更新： 2022/04/01  "
},
{
	"uri": "/tags/auto-scaling.html",
	"title": "Auto Scaling",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/expose.html",
	"title": "Expose",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/security.html",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/service.html",
	"title": "Service",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]